{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Thank you for visiting my notebook.\n\n[My discussion topic](https://www.kaggle.com/competitions/google-universal-image-embedding/discussion/359351)\n\n## Acknowledgement\n\nThis notebook is heavily based on @motono0223's work. Thank you!\nhttps://www.kaggle.com/code/motono0223/guie-clip-tensorflow-train-example\n\n## Key improvements over the original work\n\n- Input 64D vectors to ArcFace (instead of 256D ones) (~ 0.01 gain)\n- [Open CLIP ViT-H/14 on LAION-2B](https://laion.ai/blog/large-openclip/) (~ 0.08 gain)\n- Reduce LR linearly (~ 0.01 gain)\n- Finetune backbone CLIP model for 10 epochs w/ low LR w/o DA (~ 0.02 gain)\n\n## Note\n\n- I suffered from random OOM errors in submission (with CLIP H/14). I gave up more complex architecture because of this.\n- You may not be able to reproduce the same result because the random seed is not fixed.\n\n## Model\n\nAlmost the same as @motono0223's original work, except that the Dense layer's dim is 64.\n\n- for training:  \nbackbone(CLIP) + Dropout + Dense(units=64) + Arcface + Softmax (classes=17691)\n- for inference:  \nbackbone(CLIP) + Dropout + Dense(units=64) + L2Norm\n\n## Dataset for training:\n\nEventually, I used the same datasets as the original work.\nI couldn't find any other dataset which works well.","metadata":{}},{"cell_type":"markdown","source":"# Libraries","metadata":{"id":"cUF4H1xBsYb6"}},{"cell_type":"code","source":"import os\ndef is_colab_env():\n    is_colab = False\n    for k in os.environ.keys():\n        if \"COLAB\" in k:\n            is_colab = True\n            break\n    return is_colab\n\n# if google colab, install transformers and tensorflow_addons\n# (Note: please use google colab(TPU) when model is trained. \n#  On the kaggle TPU env, the module transformers.TFCLIPVisionModel couldn't be installed.)\nif is_colab_env():\n    !pip install transformers\n    !pip install tensorflow_addons\nelse: # Kaggle Notebook env\n    # for TPU\n    # https://www.kaggle.com/code/motono0223/guie-clip-tensorflow-train-example/comments#1916619\n    !pip install transformers==4.20 -U -q","metadata":{"id":"05oZ1Q5Idhj-","outputId":"548ec03d-91de-4a68-bfb9-9830ea763b5e","execution":{"iopub.status.busy":"2022-11-03T06:00:51.474507Z","iopub.execute_input":"2022-11-03T06:00:51.475067Z","iopub.status.idle":"2022-11-03T06:01:09.729960Z","shell.execute_reply.started":"2022-11-03T06:00:51.474937Z","shell.execute_reply":"2022-11-03T06:01:09.729145Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 1.12.1 requires huggingface-hub<0.1.0,>=0.0.14, but you have huggingface-hub 0.10.1 which is incompatible.\nallennlp 2.7.0 requires transformers<4.10,>=4.1, but you have transformers 4.20.0 which is incompatible.\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import CLIPProcessor, TFCLIPVisionModel, CLIPFeatureExtractor\n\nimport re\nimport os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport random\nimport math\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold, train_test_split, StratifiedKFold\nfrom tensorflow.keras import backend as K\nimport tensorflow_addons as tfa\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import normalize\nimport pickle\nimport json\nimport tensorflow_hub as tfhub\nfrom datetime import datetime\nimport gc\nimport requests\nfrom mpl_toolkits import axes_grid1","metadata":{"id":"i72153AaDJds","execution":{"iopub.status.busy":"2022-11-03T06:01:33.125170Z","iopub.execute_input":"2022-11-03T06:01:33.125792Z","iopub.status.idle":"2022-11-03T06:01:43.753699Z","shell.execute_reply.started":"2022-11-03T06:01:33.125755Z","shell.execute_reply":"2022-11-03T06:01:43.752809Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2022-11-03 06:01:34.483883: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2022-11-03 06:01:34.484007: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2022-11-03 06:01:43.308803: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2022-11-03 06:01:43.312562: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2022-11-03 06:01:43.312635: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n2022-11-03 06:01:43.312677: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (111b60362367): /proc/driver/nvidia/version does not exist\n2022-11-03 06:01:43.316392: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-11-03 06:01:43.318097: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Device","metadata":{"id":"k8BdwSO1sYcN"}},{"cell_type":"code","source":"import tensorflow as tf\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nAUTO = tf.data.experimental.AUTOTUNE\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"id":"khrTPhLcR39a","outputId":"7adb1df9-4f27-4042-c084-cfc7881b8728","execution":{"iopub.status.busy":"2022-11-03T06:01:54.119441Z","iopub.execute_input":"2022-11-03T06:01:54.119809Z","iopub.status.idle":"2022-11-03T06:02:00.001051Z","shell.execute_reply.started":"2022-11-03T06:01:54.119772Z","shell.execute_reply":"2022-11-03T06:01:59.999985Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Running on TPU  grpc://10.0.0.2:8470\n","output_type":"stream"},{"name":"stderr","text":"2022-11-03 06:01:54.127191: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n2022-11-03 06:01:54.155289: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2022-11-03 06:01:54.155349: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30020}\n2022-11-03 06:01:54.172498: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2022-11-03 06:01:54.172568: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30020}\n2022-11-03 06:01:54.173436: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:30020\n","output_type":"stream"},{"name":"stdout","text":"REPLICAS:  8\n","output_type":"stream"}]},{"cell_type":"code","source":"# If GPU instance, it makes mixed precision enable.\nif strategy.num_replicas_in_sync == 1:\n    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n    policy = mixed_precision.Policy('mixed_float16')\n    mixed_precision.set_policy(policy) ","metadata":{"id":"tKetdL8BsYcQ","execution":{"iopub.status.busy":"2022-11-03T06:02:08.202961Z","iopub.execute_input":"2022-11-03T06:02:08.203906Z","iopub.status.idle":"2022-11-03T06:02:08.209091Z","shell.execute_reply.started":"2022-11-03T06:02:08.203862Z","shell.execute_reply":"2022-11-03T06:02:08.208154Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class config:\n    SEED = 42\n\n    # pretrained model\n    RESUME = True\n    RESUME_EPOCH = 0\n    RESUME_WEIGHT = '../input/guie-clip-tf-train-models/model.H14-e100.h5'\n\n    # backbone model\n    MODEL_NAME = \"CLIP-ViT-H-14-laion2B-s32B-b79K\"\n    PRETRAINED_CLIP = f\"laion/{MODEL_NAME}\"\n    PRETRAINED_CLIP_FROM_PT = True\n    FREEZE_CLIP = False\n    IMAGE_SIZE = 224\n\n    # projection layer\n    N_CLASSES = 17691\n    EMB_DIM = 64\n    DROPOUT_RATE = 0.\n    \n    # training\n    TRAIN = True\n    SHUFFLE = True\n    # for first-round (RESUME = False, FREEZE_CLIP = True)\n    #AUGMENT = True\n    #BATCH_SIZE = 200 * strategy.num_replicas_in_sync\n    #EPOCHS = 100\n    #MAX_LR_PER_EXAMPLE = 5e-6\n    # /first-round\n    # for second-round (RESUME = True, FREEZE_CLIP = False)\n    AUGMENT = False\n    BATCH_SIZE = 25 * strategy.num_replicas_in_sync\n    EPOCHS = 1\n    MAX_LR_PER_EXAMPLE = 1e-8\n    # /second-round\n    MIN_LR_PER_EXAMPLE = 1e-9\n    save_dir = \".\"\n\n    DEBUG = False\n\n    ROOT_DIRS = [\n        \"guie-glr2021mini-tfrecords-label-10691-17690\",\n        \"guie-imagenet1k-mini1-tfrecords-label-0-999\",\n        \"guie-products10k-tfrecords-label-1000-10690\",\n        #\"guie-objectnet-224pix-tfrecords-label-0-312\",\n        #\"guie-omnibenchmark-tfrecord\",\n    ]\n\n# Function to seed everything\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)","metadata":{"id":"lCwQB_L1NGoH","outputId":"040dbaa6-54e6-4135-8251-bd779314b05f","execution":{"iopub.status.busy":"2022-11-03T06:22:52.285966Z","iopub.status.idle":"2022-11-03T06:22:52.286348Z","shell.execute_reply.started":"2022-11-03T06:22:52.286146Z","shell.execute_reply":"2022-11-03T06:22:52.286180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TFRecords","metadata":{"id":"FaUn2_W4Hm3t"}},{"cell_type":"code","source":"if is_colab_env(): # for google colab env.\n    kaggle_backet_dict = {\n        \"guie-imagenet1k-mini1-tfrecords-label-0-999\" : \"gs://kds-2d8595950771cb234df96ce51a63f9dcd7f6875446c216eb5ea81398\",\n        \"guie-products10k-tfrecords-label-1000-10690\" : \"gs://kds-bd5de1fcfc5636b47d6ed48a48794b70336cf42bd2a276c4fbcd2619\",\n        \"guie-glr2021mini-tfrecords-label-10691-17690\" : \"gs://kds-31d5ada0f474669112699c448cb8759741c402ec75aedfe1fea6fdd6\",\n        \"guie-omnibenchmark-tfrecord\" : \"gs://kds-b1f7fae3c9a8d7468f3f170254ed8d760bd3d60d6511c42e8ec42636\",\n        \"guie-objectnet-224pix-tfrecords-label-0-312\" : \"gs://kds-c1e9aafcf6753fbabd519ff9f40252645aa92604c1a5a8ee89264fbe\",\n    }\nelse: # for kaggle notebook\n    from kaggle_datasets import KaggleDatasets","metadata":{"id":"IWZk8JHMdKhG","execution":{"iopub.status.busy":"2022-11-03T06:02:19.858584Z","iopub.execute_input":"2022-11-03T06:02:19.858880Z","iopub.status.idle":"2022-11-03T06:02:19.864622Z","shell.execute_reply.started":"2022-11-03T06:02:19.858851Z","shell.execute_reply":"2022-11-03T06:02:19.863534Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_shard_suffix = '*.tfrec'\n\ntrain_set_path = []\nvalid_set_path = []\nfor ROOT_DIR in config.ROOT_DIRS:\n    if is_colab_env():\n        GCS_DS_PATH = kaggle_backet_dict[ ROOT_DIR ]\n    else:\n        GCS_DS_PATH = KaggleDatasets().get_gcs_path( ROOT_DIR )\n        \n    print( f\"\\\"{ROOT_DIR}\\\" : \\\"{GCS_DS_PATH}\\\",\" )\n    files = sorted(tf.io.gfile.glob(GCS_DS_PATH + f'/{train_shard_suffix}'))\n    assert len(files) > 0\n    # split data\n    train_set_path += random.sample(files, int( len(files) * 0.9 ) )\n    valid_set_path += [ file for file in files  if not file in train_set_path ]\n    print(ROOT_DIR, \", number of tfrecords = \", len(files))\n\ntrain_set_path = sorted( train_set_path )\nvalid_set_path = sorted( valid_set_path )\n\nprint(\"# of tfrecords for training   : \", len(train_set_path))\nprint(\"# of tfrecords for validation : \", len(valid_set_path))\n\nif config.DEBUG:\n    train_set_path = random.sample( train_set_path, 4)\n    print(\"debug: reduce training data. num=\", len(train_set_path))\n    \n    valid_set_path = train_set_path #valid_set_path[:1]\n    print(\"debug: reduce validation data. num=\", len(valid_set_path))","metadata":{"id":"T7qxi07-Hp_q","outputId":"c72e22a4-1112-49cd-b80b-4627a5162b88","execution":{"iopub.status.busy":"2022-11-03T06:02:26.524205Z","iopub.execute_input":"2022-11-03T06:02:26.524495Z","iopub.status.idle":"2022-11-03T06:02:27.733534Z","shell.execute_reply.started":"2022-11-03T06:02:26.524467Z","shell.execute_reply":"2022-11-03T06:02:27.732675Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"\"guie-glr2021mini-tfrecords-label-10691-17690\" : \"gs://kds-cd92bd8f11612bc61e0ee372382b20a102e72bb0964793da57398415\",\nguie-glr2021mini-tfrecords-label-10691-17690 , number of tfrecords =  32\n","output_type":"stream"},{"name":"stderr","text":"2022-11-03 06:02:26.882523: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n","output_type":"stream"},{"name":"stdout","text":"\"guie-imagenet1k-mini1-tfrecords-label-0-999\" : \"gs://kds-494ea3f421d87f76a9a0c96c2e0e336598e77f118ab9aedfea19a7ca\",\nguie-imagenet1k-mini1-tfrecords-label-0-999 , number of tfrecords =  50\n","output_type":"stream"},{"name":"stderr","text":"2022-11-03 06:02:27.294550: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n","output_type":"stream"},{"name":"stdout","text":"\"guie-products10k-tfrecords-label-1000-10690\" : \"gs://kds-87ae287421fa4e6de3a120cae8372fa50cf0a0ef9411d68c985e67ee\",\nguie-products10k-tfrecords-label-1000-10690 , number of tfrecords =  20\n# of tfrecords for training   :  91\n# of tfrecords for validation :  11\n","output_type":"stream"},{"name":"stderr","text":"2022-11-03 06:02:27.656570: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_num_of_image(file):\n    return int(file.split(\"/\")[-1].split(\".\")[0].split(\"-\")[-1])\n\ntrain_set_len = sum( [ get_num_of_image(file) for file in train_set_path ] )\nvalid_set_len = sum( [ get_num_of_image(file) for file in valid_set_path ] )\n\ntrain_set_len, valid_set_len","metadata":{"id":"ak_bok57zAZ_","outputId":"846c548b-3e24-4e7c-e7f3-bafc690fea7f","execution":{"iopub.status.busy":"2022-11-03T06:02:38.933015Z","iopub.execute_input":"2022-11-03T06:02:38.933374Z","iopub.status.idle":"2022-11-03T06:02:38.945811Z","shell.execute_reply.started":"2022-11-03T06:02:38.933339Z","shell.execute_reply":"2022-11-03T06:02:38.944606Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(478183, 62830)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Dataset pipeline","metadata":{"id":"aZuGi10XOuiW"}},{"cell_type":"code","source":"def deserialization_fn(serialized_example):\n    parsed_example = tf.io.parse_single_example(\n        serialized_example,\n        features={\n            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n            'image/class/label': tf.io.FixedLenFeature([], tf.int64),\n        }\n    )\n    image = tf.image.decode_jpeg(parsed_example['image/encoded'], channels=3)\n    image = tf.image.resize(image, size=(config.IMAGE_SIZE, config.IMAGE_SIZE))\n    label = tf.cast(parsed_example['image/class/label'], tf.int64)\n    image = tf.cast(image, tf.float32) / 255.0\n    return image, label","metadata":{"id":"hAmYgnXmFE3P","execution":{"iopub.status.busy":"2022-11-03T06:02:51.138322Z","iopub.execute_input":"2022-11-03T06:02:51.138615Z","iopub.status.idle":"2022-11-03T06:02:51.146722Z","shell.execute_reply.started":"2022-11-03T06:02:51.138586Z","shell.execute_reply":"2022-11-03T06:02:51.145761Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def arcface_format(image, label_group):\n    return {'inp1': image, 'inp2': label_group}, label_group\n\ndef rescale_image(image, label_group):\n    image = tf.cast(image, tf.float32) * 255.0\n    return image, label_group\n\n# Data augmentation function\ndef data_augment(image, label_group):\n    image = tf.image.random_flip_left_right(image)\n    #image = tf.image.random_flip_up_down(image)\n    image = tf.image.random_hue(image, 0.01)\n    image = tf.image.random_saturation(image, 0.70, 1.30)\n    image = tf.image.random_contrast(image, 0.80, 1.20)\n    image = tf.image.random_brightness(image, 0.10)\n    return image, label_group\n\n# Dataset to obtain backbone's inference\ndef get_backbone_inference_dataset(tfrecord_paths, cache=False, repeat=False, shuffle=False, augment=False):\n    dataset = tf.data.Dataset.from_tensor_slices(tfrecord_paths)\n    data_len = sum( [ get_num_of_image(file) for file in tfrecord_paths ] )\n    if shuffle:\n        dataset = dataset.shuffle(len(tfrecord_paths))\n    dataset = dataset.flat_map(tf.data.TFRecordDataset)\n    dataset = dataset.map(deserialization_fn, num_parallel_calls=AUTO) # image[0-1], label[0-999]\n    if shuffle:\n        dataset = dataset.shuffle(data_len//10)\n\n    if augment:\n        dataset = dataset.map(data_augment, num_parallel_calls=AUTO)  # (image, label_group) --> (image, label_group)\n    dataset = dataset.map(rescale_image, num_parallel_calls = AUTO)  # image[0-1], label[0-n_classes] --> image[0-255], label[0-n_classes]\n    dataset = dataset.map(arcface_format, num_parallel_calls=AUTO)   # (image, label_group) --> ({\"inp1\":image, \"inp2\":label_group}, label_group )\n    if repeat:\n        dataset = dataset.repeat()\n    dataset = dataset.batch(config.BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset","metadata":{"id":"UDsIA9z0NXIe","execution":{"iopub.status.busy":"2022-11-03T06:02:55.093776Z","iopub.execute_input":"2022-11-03T06:02:55.094076Z","iopub.status.idle":"2022-11-03T06:02:55.106337Z","shell.execute_reply.started":"2022-11-03T06:02:55.094048Z","shell.execute_reply":"2022-11-03T06:02:55.105041Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Viz tfrecord images","metadata":{"id":"EbjJ86S4sYci"}},{"cell_type":"code","source":"backbone_infer_dataset_encode = get_backbone_inference_dataset(train_set_path, shuffle=config.SHUFFLE, augment=config.AUGMENT)\n\nnum_cols = 3\nnum_rows = 5\nbackbone_infer_dataset_encode = backbone_infer_dataset_encode.unbatch().batch(num_cols * num_rows)\nx, y = next(iter(backbone_infer_dataset_encode))\nprint(x[\"inp1\"].shape)\n\nfig = plt.figure(figsize=(15, 15))\ngrid = axes_grid1.ImageGrid(fig, 111, nrows_ncols=(num_cols, num_rows), axes_pad=0.1)\n\nfor i, ax in enumerate(grid):\n    ax.imshow(x[\"inp1\"][i]/255)\n    ax.axis(\"off\")\n\ndel backbone_infer_dataset_encode","metadata":{"id":"QZ92qkVDHHL9","outputId":"fb631f57-c754-4bd6-d742-b88bffc9c057","execution":{"iopub.status.busy":"2022-11-08T19:51:03.118386Z","iopub.execute_input":"2022-11-08T19:51:03.118783Z","iopub.status.idle":"2022-11-08T19:51:03.220416Z","shell.execute_reply.started":"2022-11-08T19:51:03.118668Z","shell.execute_reply":"2022-11-08T19:51:03.219002Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_20/1073085281.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbackbone_infer_dataset_encode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_backbone_inference_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSHUFFLE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUGMENT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbackbone_infer_dataset_encode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackbone_infer_dataset_encode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_cols\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'get_backbone_inference_dataset' is not defined"],"ename":"NameError","evalue":"name 'get_backbone_inference_dataset' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"# Model","metadata":{"id":"dOPX4LshNXsM"}},{"cell_type":"code","source":"# Arcmarginproduct class keras layer\nclass ArcMarginProduct(tf.keras.layers.Layer):\n    '''\n    Implements large margin arc distance.\n\n    Reference:\n        https://arxiv.org/pdf/1801.07698.pdf\n        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n            blob/master/src/modeling/metric_learning.py\n    '''\n    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n                 ls_eps=0.0, **kwargs):\n\n        super(ArcMarginProduct, self).__init__(**kwargs)\n\n        self.n_classes = n_classes\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps\n        self.easy_margin = easy_margin\n        self.cos_m = tf.math.cos(m)\n        self.sin_m = tf.math.sin(m)\n        self.th = tf.math.cos(math.pi - m)\n        self.mm = tf.math.sin(math.pi - m) * m\n\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'n_classes': self.n_classes,\n            's': self.s,\n            'm': self.m,\n            'ls_eps': self.ls_eps,\n            'easy_margin': self.easy_margin,\n        })\n        return config\n\n    def build(self, input_shape):\n        super(ArcMarginProduct, self).build(input_shape[0])\n\n        self.W = self.add_weight(\n            name='W',\n            shape=(int(input_shape[0][-1]), self.n_classes),\n            initializer='glorot_uniform',\n            dtype='float32',\n            trainable=True,\n            regularizer=None)\n\n    def call(self, inputs):\n        X, y = inputs\n        y = tf.cast(y, dtype=tf.int32)\n        cosine = tf.matmul(\n            tf.math.l2_normalize(X, axis=1),\n            tf.math.l2_normalize(self.W, axis=0)\n        )\n        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = tf.where(cosine > 0, phi, cosine)\n        else:\n            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n        one_hot = tf.cast(\n            tf.one_hot(y, depth=self.n_classes),\n            dtype=cosine.dtype\n        )\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n        return output","metadata":{"id":"1LjaDRLgMjdq","execution":{"iopub.status.busy":"2022-11-03T06:03:29.959705Z","iopub.execute_input":"2022-11-03T06:03:29.960238Z","iopub.status.idle":"2022-11-03T06:03:29.978208Z","shell.execute_reply.started":"2022-11-03T06:03:29.960200Z","shell.execute_reply":"2022-11-03T06:03:29.977317Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def get_scale_layer(rescale_mode = \"tf\"):\n    # For keras_cv_attention_models module:\n    # ref: https://github.com/leondgarse/keras_cv_attention_models/blob/main/keras_cv_attention_models/imagenet/data.py\n    # ref function : init_mean_std_by_rescale_mode()\n\n    # For effV2 (21k classes) : https://github.com/leondgarse/keras_efficientnet_v2\n\n    if isinstance(rescale_mode, (list, tuple)):  # Specific mean and std\n        mean, std = rescale_mode\n    elif rescale_mode == \"torch\":\n        mean = np.array([0.485, 0.456, 0.406]) * 255.0\n        std = np.array([0.229, 0.224, 0.225]) * 255.0\n    elif rescale_mode == \"tf\":  # [0, 255] -> [-1, 1]\n        mean, std = 127.5, 127.5\n    elif rescale_mode == \"tf128\":  # [0, 255] -> [-1, 1]\n        mean, std = 128.0, 128.0\n    elif rescale_mode == \"raw01\":\n        mean, std = 0, 255.0  # [0, 255] -> [0, 1]\n    else:\n        mean, std = 0, 1  # raw inputs [0, 255]        \n    scaling_layer = keras.layers.Lambda(lambda x: ( tf.cast(x, tf.float32) - mean) / std )\n    \n    return scaling_layer\n\n\ndef get_clip_model():\n    inp = tf.keras.layers.Input(shape = [3, 224, 224]) # [B, C, H, W]\n    backbone = TFCLIPVisionModel.from_pretrained(config.PRETRAINED_CLIP,\n                                                 from_pt=config.PRETRAINED_CLIP_FROM_PT)\n    output = backbone({'pixel_values':inp}).pooler_output\n    return tf.keras.Model(inputs=[inp], outputs=[output])\n\ndef get_embedding_model():\n    #------------------\n    # Definition of placeholders\n    inp = tf.keras.layers.Input(shape = [None, None, 3], name = 'inp1')\n    label = tf.keras.layers.Input(shape = (), name = 'inp2')\n\n    # Definition of layers\n    layer_resize = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, [config.IMAGE_SIZE, config.IMAGE_SIZE]), name='resize')\n    layer_scaling = get_scale_layer(rescale_mode = \"torch\")\n    layer_permute = tf.keras.layers.Permute((3,1,2))\n    layer_backbone = get_clip_model()\n    layer_dropout = tf.keras.layers.Dropout(config.DROPOUT_RATE)\n    layer_dense_before_arcface = tf.keras.layers.Dense(config.EMB_DIM)\n    layer_margin = ArcMarginProduct(\n        n_classes = config.N_CLASSES, \n        s = 30, \n        m = 0.3, \n        name=f'head/arcface', \n        dtype='float32'\n        )\n    layer_softmax = tf.keras.layers.Softmax(dtype='float32')\n    layer_l2 = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=-1), name='embedding_norm')\n\n    #------------------\n    # Definition of entire model\n    image = layer_scaling(inp)\n    image = layer_resize(image)\n    image = layer_permute(image)\n    backbone_output = layer_backbone(image)\n    embed = layer_dropout(backbone_output)\n    embed = layer_dense_before_arcface(embed)\n    x = layer_margin([embed, label])\n    output = layer_softmax(x)\n    model = tf.keras.models.Model(inputs = [inp, label], outputs = [output]) # whole architecture\n\n    if config.FREEZE_CLIP:\n        assert model.layers[4].layers[1].layers[0].name == 'clip'\n        model.layers[4].trainable = False\n\n    opt = tf.keras.optimizers.Adam()\n    model.compile(\n        optimizer = opt,\n        loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n        metrics = [tf.keras.metrics.SparseCategoricalAccuracy(),tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)]\n        )\n\n    #------------------\n    # Definition of embedding model (for submission)\n    embed_model = keras.Sequential([\n        keras.layers.InputLayer(input_shape=(None, None, 3), dtype='uint8'),\n        layer_scaling,\n        layer_resize,\n        layer_permute,\n        layer_backbone,\n        layer_dropout,\n        layer_dense_before_arcface,\n        layer_l2,\n    ])\n\n    return model, embed_model","metadata":{"id":"jX76WJYoMgey","execution":{"iopub.status.busy":"2022-11-03T06:03:34.390984Z","iopub.execute_input":"2022-11-03T06:03:34.391303Z","iopub.status.idle":"2022-11-03T06:03:34.412582Z","shell.execute_reply.started":"2022-11-03T06:03:34.391272Z","shell.execute_reply":"2022-11-03T06:03:34.411367Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model, emb_model = get_embedding_model()\n\nif config.RESUME:\n    print(f\"load {config.RESUME_WEIGHT}\")\n    model.load_weights( config.RESUME_WEIGHT )","metadata":{"id":"5nGM8XncMglt","outputId":"a8526809-ce45-4b10-f642-a72c6014cb1e","execution":{"iopub.status.busy":"2022-11-03T06:03:39.642903Z","iopub.execute_input":"2022-11-03T06:03:39.643277Z","iopub.status.idle":"2022-11-03T06:08:09.746344Z","shell.execute_reply.started":"2022-11-03T06:03:39.643230Z","shell.execute_reply":"2022-11-03T06:08:09.744299Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/4.50k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7f60d5af47041aca589bd41d79f7120"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/3.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"270a73fd3a4f4f38944c312724139a3d"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFCLIPVisionModel: ['text_model.encoder.layers.12.layer_norm2.bias', 'text_model.encoder.layers.19.mlp.fc2.weight', 'text_model.encoder.layers.21.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.16.self_attn.v_proj.weight', 'text_model.encoder.layers.18.self_attn.out_proj.bias', 'text_model.encoder.layers.19.layer_norm2.bias', 'text_model.encoder.layers.14.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.13.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.20.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.embeddings.position_ids', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.19.self_attn.q_proj.bias', 'text_model.encoder.layers.14.self_attn.k_proj.bias', 'text_model.encoder.layers.23.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.16.self_attn.k_proj.weight', 'text_model.encoder.layers.16.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.23.layer_norm2.bias', 'text_model.encoder.layers.16.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.21.layer_norm1.bias', 'text_model.encoder.layers.18.layer_norm2.bias', 'text_model.encoder.layers.16.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.13.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.18.mlp.fc2.weight', 'text_model.encoder.layers.22.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.13.self_attn.v_proj.bias', 'text_model.encoder.layers.17.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.22.self_attn.k_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.16.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.14.self_attn.q_proj.bias', 'text_model.encoder.layers.17.mlp.fc2.weight', 'text_model.encoder.layers.23.self_attn.k_proj.weight', 'text_model.encoder.layers.23.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.22.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.19.layer_norm1.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.20.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.14.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.17.layer_norm1.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.14.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.16.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.15.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.21.self_attn.out_proj.weight', 'text_model.encoder.layers.21.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'logit_scale', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.14.mlp.fc2.bias', 'text_model.encoder.layers.19.self_attn.k_proj.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.18.mlp.fc1.weight', 'text_model.encoder.layers.22.self_attn.v_proj.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.13.self_attn.out_proj.bias', 'text_model.encoder.layers.23.self_attn.q_proj.weight', 'text_model.encoder.layers.14.mlp.fc1.weight', 'text_model.encoder.layers.21.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.18.self_attn.out_proj.weight', 'text_model.encoder.layers.20.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.22.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.20.self_attn.k_proj.bias', 'text_model.encoder.layers.15.self_attn.v_proj.bias', 'text_model.encoder.layers.17.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.15.self_attn.k_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.20.layer_norm2.weight', 'text_model.encoder.layers.15.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.13.mlp.fc1.bias', 'text_model.encoder.layers.22.self_attn.out_proj.weight', 'text_model.encoder.layers.15.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.20.layer_norm1.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.19.self_attn.out_proj.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.21.self_attn.q_proj.bias', 'text_model.encoder.layers.12.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.16.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.15.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.19.self_attn.v_proj.weight', 'text_model.encoder.layers.13.self_attn.k_proj.weight', 'text_model.encoder.layers.15.self_attn.out_proj.bias', 'text_model.encoder.layers.12.self_attn.q_proj.bias', 'text_model.encoder.layers.19.self_attn.k_proj.bias', 'text_model.encoder.layers.20.mlp.fc1.bias', 'text_model.encoder.layers.15.layer_norm2.weight', 'text_model.encoder.layers.18.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_projection.weight', 'text_model.encoder.layers.12.self_attn.v_proj.bias', 'text_model.encoder.layers.22.layer_norm2.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.17.layer_norm2.weight', 'text_model.encoder.layers.18.mlp.fc2.bias', 'text_model.encoder.layers.13.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.19.self_attn.out_proj.weight', 'text_model.encoder.layers.21.layer_norm2.weight', 'text_model.encoder.layers.20.self_attn.v_proj.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.15.self_attn.v_proj.weight', 'text_model.encoder.layers.20.self_attn.q_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.21.layer_norm2.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.16.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.22.self_attn.q_proj.weight', 'text_model.encoder.layers.12.mlp.fc2.bias', 'text_model.encoder.layers.19.mlp.fc1.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.12.self_attn.k_proj.bias', 'text_model.encoder.layers.19.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.14.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.16.mlp.fc2.weight', 'text_model.encoder.layers.21.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.12.layer_norm2.weight', 'text_model.encoder.layers.22.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.21.self_attn.v_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.13.self_attn.q_proj.bias', 'text_model.encoder.layers.12.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.15.layer_norm1.weight', 'text_model.encoder.layers.18.layer_norm1.bias', 'text_model.encoder.layers.15.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.14.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.21.self_attn.k_proj.weight', 'text_model.encoder.layers.18.self_attn.v_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.14.self_attn.v_proj.weight', 'text_model.encoder.layers.15.mlp.fc2.weight', 'text_model.encoder.layers.19.mlp.fc1.weight', 'text_model.encoder.layers.23.layer_norm1.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.16.self_attn.out_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.16.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.14.self_attn.out_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.18.self_attn.k_proj.bias', 'text_model.encoder.layers.16.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.12.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.22.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.17.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.14.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.15.mlp.fc1.weight', 'text_model.encoder.layers.14.mlp.fc2.weight', 'text_model.encoder.layers.16.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.17.mlp.fc2.bias', 'text_model.encoder.layers.13.layer_norm1.weight', 'text_model.encoder.layers.23.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.19.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.17.self_attn.q_proj.weight', 'text_model.encoder.layers.13.layer_norm2.bias', 'text_model.encoder.layers.21.self_attn.out_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.20.self_attn.v_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.12.layer_norm1.weight', 'text_model.encoder.layers.18.self_attn.k_proj.weight', 'text_model.encoder.layers.13.self_attn.v_proj.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.23.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.22.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.21.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.23.mlp.fc2.weight', 'text_model.encoder.layers.22.self_attn.k_proj.weight', 'text_model.encoder.layers.22.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.23.layer_norm1.weight', 'text_model.encoder.layers.19.self_attn.v_proj.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.15.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.12.self_attn.k_proj.weight', 'text_model.encoder.layers.13.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.19.layer_norm1.bias', 'text_model.encoder.layers.22.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.18.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'visual_projection.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.14.layer_norm1.bias', 'text_model.encoder.layers.17.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.23.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.23.mlp.fc1.weight', 'text_model.encoder.layers.22.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.13.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.12.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.17.mlp.fc1.weight', 'text_model.encoder.layers.14.self_attn.k_proj.weight', 'text_model.encoder.layers.15.self_attn.k_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.12.mlp.fc2.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.20.self_attn.q_proj.bias', 'text_model.encoder.layers.16.layer_norm1.bias', 'text_model.encoder.layers.21.mlp.fc2.bias', 'text_model.encoder.layers.18.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.17.self_attn.v_proj.weight', 'text_model.encoder.layers.13.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.15.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.20.mlp.fc1.weight', 'text_model.encoder.layers.20.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.16.mlp.fc2.bias', 'text_model.encoder.layers.21.self_attn.k_proj.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.17.self_attn.k_proj.weight', 'text_model.encoder.layers.20.self_attn.k_proj.weight', 'text_model.encoder.layers.21.self_attn.q_proj.weight', 'text_model.encoder.layers.22.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.12.self_attn.out_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.19.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.13.self_attn.out_proj.weight', 'text_model.encoder.layers.23.self_attn.out_proj.bias', 'text_model.encoder.layers.20.self_attn.out_proj.bias', 'text_model.encoder.layers.18.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.17.self_attn.out_proj.weight', 'text_model.encoder.layers.13.layer_norm1.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.17.self_attn.out_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.23.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.12.layer_norm1.bias', 'text_model.encoder.layers.20.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.18.layer_norm2.weight', 'text_model.encoder.layers.12.self_attn.v_proj.weight', 'text_model.encoder.layers.23.layer_norm2.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.17.self_attn.q_proj.bias', 'text_model.encoder.layers.17.self_attn.v_proj.bias', 'text_model.encoder.layers.18.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.14.self_attn.out_proj.weight', 'text_model.encoder.layers.23.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight']\n- This IS expected if you are initializing TFCLIPVisionModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFCLIPVisionModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFCLIPVisionModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFCLIPVisionModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"load ../input/guie-clip-tf-train-models/model.H14-e100.h5\n","output_type":"stream"}]},{"cell_type":"code","source":"model.summary()","metadata":{"id":"qSWkPesHMgpO","outputId":"b65d4347-69b9-4d35-fcc4-02cf8e1d68b7","execution":{"iopub.status.busy":"2022-11-03T06:08:09.750767Z","iopub.execute_input":"2022-11-03T06:08:09.751248Z","iopub.status.idle":"2022-11-03T06:08:09.844063Z","shell.execute_reply.started":"2022-11-03T06:08:09.751192Z","shell.execute_reply":"2022-11-03T06:08:09.843314Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Model: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninp1 (InputLayer)               [(None, None, None,  0                                            \n__________________________________________________________________________________________________\nlambda (Lambda)                 (None, None, None, 3 0           inp1[0][0]                       \n__________________________________________________________________________________________________\nresize (Lambda)                 (None, 224, 224, 3)  0           lambda[0][0]                     \n__________________________________________________________________________________________________\npermute (Permute)               (None, 3, 224, 224)  0           resize[0][0]                     \n__________________________________________________________________________________________________\nmodel (Functional)              (None, 1280)         630766080   permute[0][0]                    \n__________________________________________________________________________________________________\ndropout_32 (Dropout)            (None, 1280)         0           model[0][0]                      \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 64)           81984       dropout_32[0][0]                 \n__________________________________________________________________________________________________\ninp2 (InputLayer)               [(None,)]            0                                            \n__________________________________________________________________________________________________\nhead/arcface (ArcMarginProduct) (None, 17691)        1132224     dense[0][0]                      \n                                                                 inp2[0][0]                       \n__________________________________________________________________________________________________\nsoftmax (Softmax)               (None, 17691)        0           head/arcface[0][0]               \n==================================================================================================\nTotal params: 631,980,288\nTrainable params: 631,980,288\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"emb_model.summary()","metadata":{"id":"0a5LW0tnMgsX","outputId":"9143889a-8d13-493d-bb8a-f0323a7da97f","execution":{"iopub.status.busy":"2022-11-03T06:37:36.601739Z","iopub.execute_input":"2022-11-03T06:37:36.602176Z","iopub.status.idle":"2022-11-03T06:37:36.696433Z","shell.execute_reply.started":"2022-11-03T06:37:36.602066Z","shell.execute_reply":"2022-11-03T06:37:36.695026Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_20/1645675117.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'emb_model' is not defined"],"ename":"NameError","evalue":"name 'emb_model' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"# Scheduler","metadata":{"id":"5z_2vp1TsYcx"}},{"cell_type":"code","source":"def get_lr_callback(plot=False):\n    lr_start   = config.MIN_LR_PER_EXAMPLE * config.BATCH_SIZE\n    lr_max     = config.MAX_LR_PER_EXAMPLE * config.BATCH_SIZE\n    lr_min     = config.MIN_LR_PER_EXAMPLE * config.BATCH_SIZE\n    lr_ramp_ep = 4\n    lr_sus_ep  = 0\n    lr_decay   = 0.95\n   \n    def lrfn(epoch):\n        if config.RESUME:\n            epoch = epoch + config.RESUME_EPOCH\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        else:\n            lr = (lr_max - lr_min) * (config.EPOCHS - epoch - 1) / (config.EPOCHS - lr_ramp_ep - lr_sus_ep - 1) + lr_min\n            #lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        return lr\n        \n    if plot:\n        epochs = list(range(config.EPOCHS))\n        learning_rates = [lrfn(x) for x in epochs]\n        plt.scatter(epochs,learning_rates)\n        plt.show()\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n    return lr_callback\n\nget_lr_callback(plot=True)","metadata":{"id":"X091wmn0sYcy","outputId":"a268611b-5af9-46ce-f1e0-f2276ed8e0ff","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train entire model ","metadata":{"id":"CDmefQyfsYcz"}},{"cell_type":"code","source":"if config.TRAIN:\n    sv_loss = tf.keras.callbacks.ModelCheckpoint(\n        config.save_dir+f\"/{config.MODEL_NAME}_{config.IMAGE_SIZE}pix-emb{config.EMB_DIM}_loss.h5\", monitor='val_sparse_categorical_accuracy', verbose=1, save_best_only=True,\n        save_weights_only=True, mode='max', save_freq='epoch')\n\n    steps_per_epoch = train_set_len // config.BATCH_SIZE  // 1     # \"//10\" means that the lr is update every 0.1 epoch.\n    validation_steps = valid_set_len // config.BATCH_SIZE\n    if valid_set_len % config.BATCH_SIZE != 0:\n        validation_steps += 1\n    print(steps_per_epoch, validation_steps)\n    ds_train = get_backbone_inference_dataset(train_set_path, shuffle=config.SHUFFLE, augment=config.AUGMENT, repeat=True)\n    ds_valid = get_backbone_inference_dataset(valid_set_path, shuffle=False, augment=False, repeat=False)\n\n    history = model.fit(\n        ds_train,\n#         epochs=config.EPOCHS,\n        epochs=1,\n        callbacks=[get_lr_callback(), sv_loss],\n        steps_per_epoch=steps_per_epoch,\n        validation_data = ds_valid,\n        validation_steps = validation_steps,\n        verbose=1\n    )\n\n    # load best weight\n    model.load_weights(config.save_dir+f\"/{config.MODEL_NAME}_{config.IMAGE_SIZE}pix-emb{config.EMB_DIM}_loss.h5\")","metadata":{"id":"QSTjDBPzsYc1","outputId":"049a324f-940f-4b05-855b-e49bfad189ff","execution":{"iopub.status.busy":"2022-11-03T06:18:51.226121Z","iopub.execute_input":"2022-11-03T06:18:51.226874Z","iopub.status.idle":"2022-11-03T06:22:52.282636Z","shell.execute_reply.started":"2022-11-03T06:18:51.226828Z","shell.execute_reply":"2022-11-03T06:22:52.281090Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"2390 315\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_20/1728857232.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1103\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \"\"\"\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    294\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    508\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \"\"\"\n\u001b[1;32m   1070\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# save for debug\nemb_model.save_weights( config.save_dir+f\"/{config.MODEL_NAME}_{config.IMAGE_SIZE}pix-emb{config.EMB_DIM}_emb_model.h5\" )","metadata":{"execution":{"iopub.status.busy":"2022-10-13T20:26:18.765879Z","iopub.execute_input":"2022-10-13T20:26:18.766404Z","iopub.status.idle":"2022-10-13T20:26:29.800067Z","shell.execute_reply.started":"2022-10-13T20:26:18.766318Z","shell.execute_reply":"2022-10-13T20:26:29.799116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create submission.zip","metadata":{"id":"kG-r1le7SF9F"}},{"cell_type":"code","source":"save_locally = tf.saved_model.SaveOptions(\n    experimental_io_device='/job:localhost'\n)\nemb_model.save('./embedding_norm_model', options=save_locally)\n\nfrom zipfile import ZipFile\n\nwith ZipFile('submission.zip','w') as zip:           \n    zip.write(\n        './embedding_norm_model/saved_model.pb', \n        arcname='saved_model.pb'\n    ) \n    zip.write(\n        './embedding_norm_model/variables/variables.data-00000-of-00001', \n        arcname='variables/variables.data-00000-of-00001'\n    ) \n    zip.write(\n        './embedding_norm_model/variables/variables.index', \n        arcname='variables/variables.index'\n    )","metadata":{"id":"_eeqo14RMxEr","execution":{"iopub.status.busy":"2022-10-13T20:26:29.801979Z","iopub.execute_input":"2022-10-13T20:26:29.802259Z","iopub.status.idle":"2022-10-13T20:28:52.619301Z","shell.execute_reply.started":"2022-10-13T20:26:29.802231Z","shell.execute_reply":"2022-10-13T20:28:52.617216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"hdca\")","metadata":{"execution":{"iopub.status.busy":"2022-11-03T04:49:08.636407Z","iopub.execute_input":"2022-11-03T04:49:08.636870Z","iopub.status.idle":"2022-11-03T04:49:08.641382Z","shell.execute_reply.started":"2022-11-03T04:49:08.636846Z","shell.execute_reply":"2022-11-03T04:49:08.640226Z"},"trusted":true},"execution_count":null,"outputs":[]}]}